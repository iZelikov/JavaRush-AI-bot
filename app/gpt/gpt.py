import asyncio
from typing import Optional, List, Dict, Union

from aiogram.enums import ChatAction
from aiogram.types import Message, CallbackQuery
from openai import AsyncOpenAI, AsyncStream, RateLimitError, APITimeoutError, APIError
from openai.types.chat import ChatCompletionChunk

from gpt.clients_manager import ClientsManager
from storage.abstract_storage import AbstractStorage
from utils import logger


class GPT:

    def __init__(
            self,
            clients_manager: ClientsManager,
            storage: AbstractStorage,
            base_prompt: str = "",
            chat_gpt_clients_manager: ClientsManager = None
    ):
        self.manager = clients_manager
        self.chat_gpt_manager = chat_gpt_clients_manager or clients_manager
        self.storage = storage
        self.base_prompt = base_prompt
        self.max_tokens = 3000
        self.temperature = 0.8

    def _clear_think(self, text: str) -> str:
        return text.split('</think>')[-1].strip()

    async def _send_chat_completion(
            self,
            messages: List[Dict],
            manager: ClientsManager = None,
            max_tokens: Optional[int] = None,
            temperature: Optional[float] = None,
            stream: bool = False
    ) -> Union[str, AsyncStream[ChatCompletionChunk]]:
        manager = manager or self.manager
        client = manager.get_client().client
        model = manager.get_client().model
        max_tokens = max_tokens or self.max_tokens
        temperature = temperature or self.temperature

        try:
            if not stream:
                response = await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
                answer_text = response.choices[0].message.content.strip()
                return self._clear_think(answer_text)
            else:
                return await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=True
                )

        except RateLimitError:
            logger.warning(f"ğŸ˜¤ ĞĞµÑ‚Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² - Ğ½ĞµÑ‚Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ¾Ğ²! GPTClient {manager.get_client().name} Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ» Ğ»Ğ¸Ğ¼Ğ¸Ñ‚")
            manager.next_client()
            return 'ERROR: Ğ‘Ñ€Ğ°Ñ‚Ğ°Ğ½, GPT Ñ‚Ğ¾ĞºĞµĞ½ ÑĞ»ĞµĞ³ĞºĞ° Ğ¿Ñ€Ğ¾Ñ‚ÑƒÑ…, Ñ‚Ğ¾ Ğ±Ğ¸ÑˆÑŒ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ» Ğ»Ğ¸Ğ¼Ğ¸Ñ‚. ĞĞ¾ Ğ½Ğµ Ğ±Ğ¾Ğ¸ÑÑŒ, Ñƒ Ğ¼ĞµĞ½Ñ Ğ·Ğ°Ğ¿Ğ°ÑĞ½Ñ‹Ğµ ĞµÑÑ‚ÑŒ. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ. Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ¿Ñ€Ğ¾ĞºĞ°Ñ‚Ğ¸Ñ‚ - Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ¸ Ğ´Ğ¾ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°.'

        except APITimeoutError:
            logger.error("GPT API timeout")
            return 'ERROR: GPT Ğ¼Ğ¾Ğ»Ñ‡Ğ¸Ñ‚ ĞºĞ°Ğº Ñ€Ñ‹Ğ±Ğ° Ğ¾Ğ± Ğ»Ñ‘Ğ´. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ Ğ¿Ğ¾Ğ·Ğ¶Ğµ.'

        except APIError as e:
            logger.error(f"OpenAI API Error: {e}")
            return 'ERROR: Ğ­Ğ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ±Ğ¾Ğ»Ğ²Ğ°Ğ½ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚, Ñ‡Ñ‚Ğ¾ Ñƒ Ñ‚ĞµĞ±Ñ GPT API Ğ½Ğµ Ñ‚Ğ°ĞºĞ¾Ğ¹ ĞºĞ°Ğº Ñƒ Ğ½ĞµĞ³Ğ¾.'

        except Exception as e:
            logger.exception(f"ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğº GPT: {e}")
            return 'ERROR: ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ½ĞµĞ²ĞµĞ´Ğ¾Ğ¼Ğ°Ñ Ñ„Ğ¸Ğ³Ğ½Ñ... GPT ÑƒÑˆÑ‘Ğ» Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·.'

    async def _handle_stream(
            self,
            stream: AsyncStream[ChatCompletionChunk],
            output_message: Message = None) -> str:

        full_text_parts = []
        buffer = []

        if output_message:
            current_message = await output_message.edit_text('...')
            last_update = asyncio.get_event_loop().time()
            update_interval = 1

            async for chunk in stream:
                part = chunk.choices[0].delta.content
                if part is None:
                    continue

                buffer.append(part)
                full_text_parts.append(part)

                current_time = asyncio.get_event_loop().time()
                if current_time - last_update >= update_interval and buffer:
                    current_message = await self._send_part(current_message, buffer)
                    buffer = []
                    last_update = current_time

            if buffer:
                current_message = await self._send_part(current_message, buffer)

            full_text = ''.join(full_text_parts)
        else:
            async for chunk in stream:
                part = chunk.choices[0].delta.content
                if part:
                    full_text_parts.append(part)
            full_text = ''.join(full_text_parts)
        return full_text

    async def _send_part(self, message: Message, buffer: list[str]) -> Message:
        if not buffer:
            return message
        max_length = 3500
        old_text = message.text
        new_part = ''.join(buffer)
        if new_part == '':
            return message

        if len(old_text) > max_length:
            loading_str = '\n\n=+=+= LOADING =+=+=\n'
            if not loading_str in old_text:
                old_text += loading_str
            new_part = '#'

        try:
            new_text = f'{old_text}{new_part}'
            return await message.edit_text(new_text, parse_mode=None)

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: {e}")
            return message

    async def _get_text_from_stream(self,
                                    messages: list[dict],
                                    output_message: Message = None):
        response_stream = await self._send_chat_completion(messages=messages, stream=True)

        if isinstance(response_stream, str):
            return response_stream

        response_text = await self._handle_stream(response_stream, output_message)
        return response_text

    async def dialog(self,
                     user_message: Message | CallbackQuery,
                     prompt: str = "",
                     text: str = "",
                     output_message: Message = None) -> str:

        user_id = user_message.from_user.id
        if isinstance(user_message, CallbackQuery):
            user_message = user_message.message
        request_text = text or user_message.text or user_message.caption or ""

        history = await self.storage.get_history(user_id)

        messages = [
            {"role": "system", "content": f"{self.base_prompt}\n{prompt}"},
            *history,
            {"role": "user", "content": request_text}
        ]

        await user_message.bot.send_chat_action(chat_id=user_message.chat.id, action=ChatAction.TYPING)

        response_text = await self._get_text_from_stream(messages, output_message)

        history += [
            {"role": "user", "content": request_text},
            {"role": "assistant", "content": response_text}
        ]
        await self.storage.save_history(user_id, history)
        return response_text

    async def ask_once(self,
                       user_message: Message | CallbackQuery,
                       prompt: str = "",
                       text: str = "",
                       output_message: Message = None) -> str:
        if isinstance(user_message, CallbackQuery):
            user_message = user_message.message
        request_text = text or user_message.text or user_message.caption or ""

        messages = [
            {"role": "system", "content": self.base_prompt + prompt},
            {"role": "user", "content": request_text}
        ]
        await user_message.bot.send_chat_action(chat_id=user_message.chat.id, action=ChatAction.TYPING)

        response_text = await self._get_text_from_stream(messages, output_message)

        return response_text

    async def ask_image(
            self,
            img_url,
            prompt: str = "ĞĞ¿Ğ¸ÑˆĞ¸ Ğ²ÑÑ‘ Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´Ğ¸ÑˆÑŒ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞµ",
    ) -> str:
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"{prompt}"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"{img_url}"
                        }
                    }
                ]
            }
        ]

        response = await self._send_chat_completion(
            messages=messages,
            manager=self.chat_gpt_manager,
            stream=False
        )
        return response
