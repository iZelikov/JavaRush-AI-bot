import asyncio
from typing import List, Dict, Union

import httpx
from aiogram.enums import ChatAction
from aiogram.types import Message, CallbackQuery
from openai import AsyncStream, RateLimitError, APITimeoutError, APIError
from openai.types.chat import ChatCompletionChunk

from gpt.clients_manager import ClientsManager
from storage.abstract_storage import AbstractStorage
from utils import logger


class GPT:

    def __init__(
            self,
            clients_manager: ClientsManager,
            storage: AbstractStorage,
            base_prompt: str = "",
            chat_gpt_clients_manager: ClientsManager = None
    ):
        self.manager = clients_manager
        self.chat_gpt_manager = chat_gpt_clients_manager or clients_manager
        self.storage = storage
        self.base_prompt = base_prompt
        self.max_tokens = 3000
        self.temperature = 0.8
        self.last_empty_client = None

    @staticmethod
    def _clear_think(text: str) -> str:
        return text.split('</think>')[-1].strip()

    async def _try_next_client(
            self,
            messages: List[Dict],
            manager: ClientsManager = None,
            stream: bool = False
    ) -> Union[str, AsyncStream[ChatCompletionChunk]]:
        if self.last_empty_client is None:
            self.last_empty_client = manager.get_client()
        elif self.last_empty_client == manager.get_client():
            self.last_empty_client = None
            return 'ERROR: Ğ‘Ñ€Ğ°Ñ‚Ğ°Ğ½, GPT Ñ‚Ğ¾ĞºĞµĞ½ ÑĞ»ĞµĞ³ĞºĞ° Ğ¿Ñ€Ğ¾Ñ‚ÑƒÑ…, Ñ‚Ğ¾ Ğ±Ğ¸ÑˆÑŒ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ» Ğ»Ğ¸Ğ¼Ğ¸Ñ‚. ĞĞ±Ğ¾Ğ¶Ğ´Ğ¸ Ğ´Ğ¾ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°.'
        manager.next_client()
        return await self._send_chat_completion(messages, manager, stream)

    async def _send_chat_completion(
            self,
            messages: List[Dict],
            manager: ClientsManager = None,
            stream: bool = False
    ) -> Union[str, AsyncStream[ChatCompletionChunk]]:
        manager = manager or self.manager
        client = manager.get_client().client
        model = manager.get_client().model
        max_tokens = self.max_tokens
        temperature = self.temperature

        try:
            if not stream:
                response = await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
                answer_text = response.choices[0].message.content.strip()
                self.last_empty_client = None
                return self._clear_think(answer_text)
            else:
                answer_stream = await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=True
                )
                self.last_empty_client = None
                return answer_stream

        except RateLimitError:
            logger.warning(f"ğŸ˜¤ ĞĞµÑ‚Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² - Ğ½ĞµÑ‚Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ¾Ğ²! GPTClient {manager.get_client().name} Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ» Ğ»Ğ¸Ğ¼Ğ¸Ñ‚")
            return await self._try_next_client(messages, manager, stream)

        except APITimeoutError:
            logger.error("GPT API timeout")
            return 'ERROR: GPT Ğ¼Ğ¾Ğ»Ñ‡Ğ¸Ñ‚ ĞºĞ°Ğº Ñ€Ñ‹Ğ±Ğ° Ğ¾Ğ± Ğ»Ñ‘Ğ´. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ Ğ¿Ğ¾Ğ·Ğ¶Ğµ.'

        except APIError as e:
            logger.error(f"OpenAI API Error: {e}")
            return 'ERROR: Ğ­Ğ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ±Ğ¾Ğ»Ğ²Ğ°Ğ½ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚, Ñ‡Ñ‚Ğ¾ Ñƒ Ñ‚ĞµĞ±Ñ GPT API Ğ½Ğµ Ñ‚Ğ°ĞºĞ¾Ğ¹ ĞºĞ°Ğº Ñƒ Ğ½ĞµĞ³Ğ¾.'

        except Exception as e:
            logger.exception(f"ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğº GPT: {e}")
            return 'ERROR: ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ½ĞµĞ²ĞµĞ´Ğ¾Ğ¼Ğ°Ñ Ñ„Ğ¸Ğ³Ğ½Ñ... GPT ÑƒÑˆÑ‘Ğ» Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·.'

    @staticmethod
    def _get_username_prompt(message: Message | CallbackQuery):
        username = message.from_user.username or ""
        fullname = message.from_user.full_name or username
        prompt  = f"""
        ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚ '{fullname}'.
        Ğ”ĞµĞ»Ğ°Ğ¹ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼ÑƒÑĞ¾Ñ€Ğ°Ğ¼ Ğ½Ğµ ÑĞ»Ğ¸Ğ²Ğ°Ğ¹.
        ĞœĞ¾Ğ¶ĞµÑˆÑŒ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ» Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğº Ğ½ĞµĞ¼Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ.
        Ğ˜Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ğ°Ğ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ³Ğ¾ Ñ‡Ñ‘Ñ‚ĞºÑƒÑ Ğ³Ğ¾Ğ¿Ğ½Ğ¸Ñ†ĞºÑƒÑ ĞºĞ»Ğ¸ĞºÑƒÑ…Ñƒ, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ¼ĞµĞ½Ğ¸.
        Ğ¡ĞµĞ±Ğµ Ñ‚Ğ¾Ğ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑˆÑŒ Ğ³Ñ€Ğ¾Ğ·Ğ½ÑƒÑ ĞºĞ»Ğ¸ĞºÑƒÑ…Ñƒ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ, Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ ĞµÑ‘ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ… Ğ¸ Ğ´Ñ€Ğ°ĞºĞ°Ñ….
        Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸ Ğ½Ğµ Ğ·Ğ°Ğ±ÑƒĞ´ÑŒ Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ğ°Ğ» Ğ¸ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ¿ÑƒÑ‚Ğ°Ğ¹! 
        Ğ˜ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ĞºĞ»Ğ¸Ñ‡ĞºÑƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ· Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸. 
        Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½ĞµĞ¼Ñƒ Ğ¿Ğ¾-Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼Ñƒ Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ»Ğ¸Ñ‡ĞºÑƒ. 
        Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ĞµÑ‘ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼.             
        """
        return prompt

    async def _handle_stream(
            self,
            stream: AsyncStream[ChatCompletionChunk],
            output_message: Message = None) -> str:

        full_text_parts = []
        buffer = []
        error_occurred = False

        try:
            if output_message:
                current_message = await output_message.edit_text('...')
                last_update = asyncio.get_event_loop().time()
                update_interval = 2

                try:
                    async for chunk in stream:
                        part = chunk.choices[0].delta.content
                        if part is None:
                            continue

                        buffer.append(part)
                        full_text_parts.append(part)

                        current_time = asyncio.get_event_loop().time()
                        if current_time - last_update >= update_interval and buffer:
                            current_message = await self._send_part(current_message, buffer)
                            buffer = []
                            last_update = current_time

                except (httpx.RemoteProtocolError, httpx.ReadError) as e:
                    logger.warning(f"Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾: {e}")
                    error_occurred = True

                if buffer:
                    current_message = await self._send_part(current_message, buffer)

                if error_occurred:
                    try:
                        text = current_message.text or ''
                        if text:
                            text += "\n\nâš ï¸ Ğ¢ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆÑ‘Ğ» Ğ¾Ğ±Ñ€Ñ‹Ğ² ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ cĞµÑ€Ğ²Ğ°ĞºĞ¾Ğ¼ GPT... ĞŸÑ€Ğ¸ÑˆĞ»Ğ¾ÑÑŒ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ."
                            await current_message.edit_text(text)
                    except Exception as e:
                        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ¾Ğ±Ñ€Ñ‹Ğ²Ğµ: {e}")

            else:
                try:
                    async for chunk in stream:
                        part = chunk.choices[0].delta.content
                        if part:
                            full_text_parts.append(part)
                except (httpx.RemoteProtocolError, httpx.ReadError) as e:
                    logger.warning(f"Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾: {e}")
                    error_occurred = True

            full_text = ''.join(full_text_parts)
            if error_occurred and not full_text:
                full_text = "ERROR: Ğ¡ĞµÑ€Ğ²Ğ°Ğº GPT Ğ²Ğ¾Ğ¾Ğ±Ñ‰Ğµ Ğ½Ğµ Ğ¾Ñ‚Ğ´ÑƒĞ¿Ğ»ÑĞµÑ‚! ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ·, Ğ²Ğ´Ñ€ÑƒĞ³ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚."

            return full_text

        except Exception as e:
            logger.error(f"ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² _handle_stream: {e}")
            return "ERROR: ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ½ĞµĞ²ĞµĞ´Ğ¾Ğ¼Ğ°Ñ Ñ„Ğ¸Ğ³Ğ½Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°."

    async def _send_part(self, message: Message, buffer: list[str]) -> Message:
        if not buffer:
            return message
        max_length = 3500
        old_text = message.text
        new_part = ''.join(buffer)
        if new_part == '':
            return message

        if len(old_text) > max_length:
            loading_str = '\n\n=+=+= LOADING =+=+=\n'
            if not loading_str in old_text:
                old_text += loading_str
            new_part = '#'

        try:
            new_text = f'{old_text}{new_part}'
            return await message.edit_text(new_text, parse_mode=None)

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: {e}")
            return message

    async def _get_text_from_stream(self,
                                    messages: list[dict],
                                    output_message: Message = None):
        response_stream = await self._send_chat_completion(messages=messages, stream=True)

        if isinstance(response_stream, str):
            return response_stream

        response_text = await self._handle_stream(response_stream, output_message)
        return response_text

    async def dialog(self,
                     user_message: Message | CallbackQuery,
                     prompt: str = "",
                     text: str = "",
                     output_message: Message = None) -> str:

        user_id = user_message.from_user.id
        username_prompt = self._get_username_prompt(user_message)

        if isinstance(user_message, CallbackQuery):
            user_message = user_message.message
        request_text = text or user_message.text or user_message.caption or ""

        history = await self.storage.get_history(user_id)
        messages = [
            {"role": "system", "content": f"{self.base_prompt}\n{username_prompt}\n{prompt}"},
            *history,
            {"role": "user", "content": request_text}
        ]

        await user_message.bot.send_chat_action(chat_id=user_message.chat.id, action=ChatAction.TYPING)

        response_text = await self._get_text_from_stream(messages, output_message)

        history += [
            {"role": "user", "content": request_text},
            {"role": "assistant", "content": response_text}
        ]
        await self.storage.save_history(user_id, history)
        return response_text

    async def ask_once(self,
                       user_message: Message | CallbackQuery,
                       prompt: str = "",
                       text: str = "",
                       output_message: Message = None) -> str:
        if isinstance(user_message, CallbackQuery):
            user_message = user_message.message
        request_text = text or user_message.text or user_message.caption or ""

        messages = [
            {"role": "system", "content": self.base_prompt + prompt},
            {"role": "user", "content": request_text}
        ]
        await user_message.bot.send_chat_action(chat_id=user_message.chat.id, action=ChatAction.TYPING)

        response_text = await self._get_text_from_stream(messages, output_message)

        return response_text

    async def ask_image(
            self,
            img_url,
            prompt: str = "ĞĞ¿Ğ¸ÑˆĞ¸ Ğ²ÑÑ‘ Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´Ğ¸ÑˆÑŒ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞµ",
    ) -> str:
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"{prompt}"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"{img_url}"
                        }
                    }
                ]
            }
        ]

        response = await self._send_chat_completion(
            messages=messages,
            manager=self.chat_gpt_manager,
            stream=False
        )
        return response
